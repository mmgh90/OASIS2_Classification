{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db9609a-4ebd-49ed-b21e-9c74e3ca1d0c",
   "metadata": {},
   "source": [
    "# Preprocessing: OASIS-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898754a-5b9e-4019-a2ee-c94b8d387b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = (\"PATH_TO_OASIS2_DATASET\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Remove columns\n",
    "df = df.drop(columns=['Hand', 'MRI ID', 'Visit', 'MR Delay'])\n",
    "\n",
    "# Impute missing values for 'SES' using the median (as they are integer-valued data)\n",
    "df[['SES', 'MMSE']] = df[['SES', 'MMSE']].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Check for any remaining missing values in each column\n",
    "print(missing_values_count = df.isnull().sum())\n",
    "\n",
    "# Selecting numeric columns for normalization\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Applying normalization\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Encoding 'Subject ID' using Label Encoding \n",
    "label_encoder = LabelEncoder()\n",
    "df['Subject ID'] = label_encoder.fit_transform(df['Subject ID'])\n",
    "\n",
    "# Encoding 'M/F' using binary encoding\n",
    "df['M/F'] = df['M/F'].map({'M': 0, 'F': 1})\n",
    "\n",
    "# Save the preprocessed data\n",
    "output_file_path = (\"PATH_TO_LOCATION\")\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f96c5-88ec-4f84-a33a-9b76efec2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = (\"PATH_TO_OASIS2_DATASET\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(columns=['Hand', 'MRI ID'])\n",
    "\n",
    "# Display initial data information\n",
    "print(\"Initial data shape:\", df.shape)\n",
    "print(\"Columns in the dataset:\", df.columns.tolist())\n",
    "\n",
    "# Impute missing values for 'SES' and 'MMSE' using the median\n",
    "df[['SES', 'MMSE']] = df[['SES', 'MMSE']].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column after imputation:\\n\", missing_values_count)\n",
    "\n",
    "# One-Hot Encode \n",
    "df = pd.get_dummies(df, columns=['M/F'], drop_first=True) \n",
    "\n",
    "# Encode 'Group' \n",
    "label_encoder_group = LabelEncoder()\n",
    "df['Group_encoded'] = label_encoder_group.fit_transform(df['Group'])\n",
    "\n",
    "# Print the mapping of classes\n",
    "print(\"\\nGroup classes and their labels:\", dict(zip(label_encoder_group.classes_, \n",
    "                                                    label_encoder_group.transform(label_encoder_group.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd5a04-cc32-41f5-bf0a-63b8e537687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by 'Subject ID' and 'Visit'\n",
    "df = df.sort_values(by=['Subject ID', 'Visit'])\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# Age features\n",
    "df['Age_at_Baseline'] = df.groupby('Subject ID')['Age'].transform('first')\n",
    "df['Age_Difference'] = df['Age'] - df['Age_at_Baseline']\n",
    "\n",
    "# MMSE features\n",
    "df['MMSE_Change'] = df.groupby('Subject ID')['MMSE'].diff().fillna(0)\n",
    "df['MMSE_at_Baseline'] = df.groupby('Subject ID')['MMSE'].transform('first')\n",
    "df['Cumulative_MMSE_Change'] = df['MMSE'] - df['MMSE_at_Baseline']\n",
    "df['Days_Since_Baseline'] = df.groupby('Subject ID')['MR Delay'].transform(lambda x: x - x.min())\n",
    "df['MMSE_Rate_of_Change'] = df['Cumulative_MMSE_Change'] / df['Days_Since_Baseline'].replace(0, np.nan)\n",
    "df['MMSE_Rate_of_Change'] = df['MMSE_Rate_of_Change'].fillna(0)\n",
    "\n",
    "# CDR features\n",
    "df['CDR_Change'] = df.groupby('Subject ID')['CDR'].diff().fillna(0)\n",
    "df['CDR_at_Baseline'] = df.groupby('Subject ID')['CDR'].transform('first')\n",
    "df['Cumulative_CDR_Change'] = df['CDR'] - df['CDR_at_Baseline']\n",
    "\n",
    "# nWBV features\n",
    "df['nWBV_Change'] = df.groupby('Subject ID')['nWBV'].diff().fillna(0)\n",
    "df['nWBV_at_Baseline'] = df.groupby('Subject ID')['nWBV'].transform('first')\n",
    "df['Cumulative_nWBV_Change'] = df['nWBV'] - df['nWBV_at_Baseline']\n",
    "df['nWBV_Rate_of_Change'] = df['Cumulative_nWBV_Change'] / df['Days_Since_Baseline'].replace(0, np.nan)\n",
    "df['nWBV_Rate_of_Change'] = df['nWBV_Rate_of_Change'].fillna(0)\n",
    "\n",
    "# SES features\n",
    "df['SES_at_Baseline'] = df.groupby('Subject ID')['SES'].transform('first')\n",
    "df['SES_Change'] = df['SES'] - df['SES_at_Baseline']\n",
    "\n",
    "# EDUC features\n",
    "df['EDUC_at_Baseline'] = df.groupby('Subject ID')['EDUC'].transform('first')\n",
    "df['EDUC_Change'] = df['EDUC'] - df['EDUC_at_Baseline']\n",
    "\n",
    "# Time Since Last Visit\n",
    "df['Time_Since_Last_Visit'] = df.groupby('Subject ID')['MR Delay'].diff().fillna(0)\n",
    "\n",
    "# eTIV features\n",
    "df['eTIV_Change'] = df.groupby('Subject ID')['eTIV'].diff().fillna(0)\n",
    "df['eTIV_at_Baseline'] = df.groupby('Subject ID')['eTIV'].transform('first')\n",
    "df['Cumulative_eTIV_Change'] = df['eTIV'] - df['eTIV_at_Baseline']\n",
    "df['eTIV_Rate_of_Change'] = df['Cumulative_eTIV_Change'] / df['Days_Since_Baseline'].replace(0, np.nan)\n",
    "df['eTIV_Rate_of_Change'] = df['eTIV_Rate_of_Change'].fillna(0)\n",
    "\n",
    "# ASF features\n",
    "df['ASF_Change'] = df.groupby('Subject ID')['ASF'].diff().fillna(0)\n",
    "df['ASF_at_Baseline'] = df.groupby('Subject ID')['ASF'].transform('first')\n",
    "df['Cumulative_ASF_Change'] = df['ASF'] - df['ASF_at_Baseline']\n",
    "df['ASF_Rate_of_Change'] = df['Cumulative_ASF_Change'] / df['Days_Since_Baseline'].replace(0, np.nan)\n",
    "df['ASF_Rate_of_Change'] = df['ASF_Rate_of_Change'].fillna(0)\n",
    "\n",
    "# Features to include\n",
    "feature_columns = [\n",
    "    'Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF', 'M/F_M',\n",
    "    'Age_Difference', 'MMSE_Change', 'Cumulative_MMSE_Change', 'MMSE_Rate_of_Change',\n",
    "    'CDR_Change', 'Cumulative_CDR_Change',\n",
    "    'nWBV_Change', 'Cumulative_nWBV_Change', 'nWBV_Rate_of_Change',\n",
    "    'SES_Change', 'EDUC_Change', 'Time_Since_Last_Visit', 'Days_Since_Baseline',\n",
    "    'eTIV_Change', 'Cumulative_eTIV_Change', 'eTIV_Rate_of_Change',\n",
    "    'ASF_Change', 'Cumulative_ASF_Change', 'ASF_Rate_of_Change'\n",
    "]\n",
    "\n",
    "# Prepare feature matrix X and target vector y\n",
    "X = df[feature_columns]\n",
    "y = df['Group_encoded']\n",
    "groups = df['Subject ID']\n",
    "\n",
    "# Check for missing values in features\n",
    "missing_values_features = X.isnull().sum()\n",
    "print(\"\\nMissing values in features:\\n\", missing_values_features)\n",
    "\n",
    "# Impute missing values in features (if any)\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Examine the variability of SES_Change and EDUC_Change\n",
    "print(\"\\nUnique values in SES_Change:\", df['SES_Change'].unique())\n",
    "print(\"Unique values in EDUC_Change:\", df['EDUC_Change'].unique())\n",
    "\n",
    "# Check if EDUC_Change & SES have any variability and remove features with no variability\n",
    "for feature in ['SES_Change', 'EDUC_Change']:\n",
    "    if df[feature].nunique() <= 1:\n",
    "        print(f\"\\n{feature} has no variability and will be removed from the feature set.\")\n",
    "        feature_columns.remove(feature)\n",
    "        X_imputed = X_imputed.drop(columns=[feature])\n",
    "    else:\n",
    "        print(f\"\\n{feature} has variability and will be included in the feature set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c9120-5113-4f1f-9957-4d62a115e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature set and target to a CSV file\n",
    "\n",
    "# Combine features and target into one DataFrame\n",
    "processed_data = X_imputed.copy()\n",
    "processed_data['Target'] = y.values  # Ensure alignment\n",
    "\n",
    "# Optionally add 'Subject ID' and 'Visit' columns\n",
    "processed_data['Subject ID'] = df['Subject ID'].values\n",
    "processed_data['Visit'] = df['Visit'].values\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = (\"PATH_TO_LOCATION\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "processed_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"\\nProcessed dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31bf1ad-4038-4b84-ae17-c768ef68570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update numerical features list\n",
    "numerical_features = [col for col in feature_columns if col != 'M/F_M']\n",
    "\n",
    "# Define ColumnTransformer for scaling\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed_array = preprocessor.fit_transform(X_imputed)\n",
    "\n",
    "# Get the names of the features after transformation\n",
    "feature_names_transformed = numerical_features + ['M/F_M']\n",
    "\n",
    "# Convert the array back to DataFrame\n",
    "X_processed = pd.DataFrame(X_processed_array, columns=feature_names_transformed)\n",
    "\n",
    "print('Ready for classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
