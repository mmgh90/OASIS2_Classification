{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9548483-9cf6-4d60-8769-f8db94a4e843",
   "metadata": {},
   "source": [
    "# LGBM Model for OASIS-2 Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd389c-ddde-4d98-8b17-3331825b4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Import Libraries\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, precision_recall_curve,\n",
    "    auc, confusion_matrix\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.combine import SMOTETomek\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightgbm import LGBMClassifier\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Configuration object for hyperparameters and file paths\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "class Config:\n",
    "    # File paths\n",
    "    BASE_PATH = (\"PATH_TO_PREPROCESSED_OASIS2_DATASET\")\n",
    "    PROCESSED_DATA_PATH = os.path.join(BASE_PATH, '0processed_dataset.csv')\n",
    "    MODEL_SAVE_PATH = os.path.join(BASE_PATH, 'best_lgbm_model.pkl')\n",
    "    SEARCH_RESULTS_PATH = os.path.join(BASE_PATH, 'hyperparameter_search_results_lgbm.csv')\n",
    "    FEATURE_IMPORTANCES_PATH = os.path.join(BASE_PATH, 'feature_importances_lgbm.csv')\n",
    "    FEATURE_IMPORTANCES_PLOT_PATH = os.path.join(BASE_PATH, 'feature_importances_lgbm_plot.png')\n",
    "    TRAIN_PREDICTIONS_PATH = os.path.join(BASE_PATH, 'train_dataset_with_predictions_lgbm.csv')\n",
    "    TEST_PREDICTIONS_PATH = os.path.join(BASE_PATH, 'test_dataset_with_predictions_lgbm.csv')\n",
    "    METRICS_SAVE_PATH = os.path.join(BASE_PATH, 'model_evaluation_metrics_lgbm.csv')\n",
    "    CV_RESULTS_PATH = os.path.join(BASE_PATH, 'cross_validation_results_lgbm.csv')\n",
    "    LABEL_ENCODER_PATH = os.path.join(BASE_PATH, 'label_encoder.pkl')\n",
    "    FEATURE_NAMES_PATH = os.path.join(BASE_PATH, 'feature_names.csv')\n",
    "\n",
    "    # Hyperparameters\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    N_ESTIMATORS_RANGE = (5, 100)  \n",
    "    LEARNING_RATE_RANGE = (0.01, 0.3)  \n",
    "    MAX_DEPTH_RANGE = (3, 10)  \n",
    "    NUM_LEAVES_RANGE = (20, 100)  \n",
    "    MIN_DATA_IN_LEAF_RANGE = (10, 100)  \n",
    "    FEATURE_FRACTION_RANGE = (0.6, 1.0)  \n",
    "    ACCEPTABLE_TRAIN_AUC_RANGE = #(, )\n",
    "    ACCEPTABLE_TEST_AUC_RANGE = #(, )\n",
    "    MAX_TRIALS = 100\n",
    "    MIN_RECALL_TEST = 0.90\n",
    "    CV_FOLDS = 5  \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Load, Preprocess & Split the Data\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the processed dataset from a CSV file.\"\"\"\n",
    "    logger.info(f\"Loading data from {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Data loaded successfully with shape {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Separate features and target.\"\"\"\n",
    "    logger.info(\"Preprocessing data\")\n",
    "    X = df.drop(columns=['Target', 'Subject ID', 'Visit', 'Time_Since_Last_Visit', 'Days_Since_Baseline',\n",
    "                         'CDR', 'CDR_Change', 'Cumulative_CDR_Change', 'MMSE_Rate_of_Change', 'Age_Difference',\n",
    "                         'MMSE_Change', 'Cumulative_MMSE_Change', 'nWBV', 'nWBV_Rate_of_Change',\n",
    "                         'Cumulative_nWBV_Change', 'eTIV_Change', 'Cumulative_eTIV_Change', 'eTIV_Rate_of_Change', 'M/F_M'])  \n",
    "    \n",
    "    y = df['Target']\n",
    "    groups = df['Subject ID']\n",
    "    return X, y, groups\n",
    "\n",
    "def split_data(X, y, groups, test_size, random_state):\n",
    "    \"\"\"Split data into training and testing sets using GroupShuffleSplit.\"\"\"\n",
    "    logger.info(\"Splitting data into training and testing sets\")\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    for train_idx, test_idx in gss.split(X, y, groups=groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        groups_train, groups_test = groups.iloc[train_idx], groups.iloc[test_idx]\n",
    "    logger.info(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, groups_train, groups_test, train_idx, test_idx\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Resampling to Handle Class Imbalance\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def compute_class_weights(y):\n",
    "    \"\"\"Compute class weights to handle class imbalance.\"\"\"\n",
    "    logger.info(\"Computing class weights\")\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    class_weights_dict = dict(zip(classes, class_weights))\n",
    "    logger.info(f\"Class weights: {class_weights_dict}\")\n",
    "    return class_weights_dict\n",
    "\n",
    "def apply_smote_tomek(X_train, y_train, random_state):\n",
    "    \"\"\"Apply SMOTE with Tomek Links to balance the training data.\"\"\"\n",
    "    logger.info(\"Applying SMOTE with Tomek Links to the training data\")\n",
    "    smote_tomek = SMOTETomek(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "    logger.info(f\"Resampled training data shape: {X_resampled.shape}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Random Search to Find Hyperparameters\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def random_hyperparameter_search(\n",
    "    X_train, y_train, X_test, y_test, class_weights, config\n",
    "):\n",
    "    \"\"\"Perform random hyperparameter search and model evaluation using LightGBM in a Pipeline.\"\"\"\n",
    "    logger.info(\"Starting random hyperparameter search with LightGBM in Pipeline\")\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_auc = 0\n",
    "    results = []\n",
    "    train_aucs = []\n",
    "    test_aucs = []\n",
    "    train_recalls = []\n",
    "    test_recalls = []\n",
    "    criteria_met = False\n",
    "\n",
    "    # Define columns to scale and pass through\n",
    "    columns_to_scale = [col for col in X_train.columns if col not in ['M/F_M', 'EDUC', 'SES']]\n",
    "    columns_to_passthrough = ['M/F_M', 'EDUC', 'SES']\n",
    "\n",
    "    for trial in range(1, config.MAX_TRIALS + 1):\n",
    "        # Randomly sample hyperparameters\n",
    "        n_estimators = random.randint(config.N_ESTIMATORS_RANGE[0], config.N_ESTIMATORS_RANGE[1])\n",
    "        learning_rate = random.uniform(config.LEARNING_RATE_RANGE[0], config.LEARNING_RATE_RANGE[1])\n",
    "        max_depth = random.randint(config.MAX_DEPTH_RANGE[0], config.MAX_DEPTH_RANGE[1])\n",
    "        num_leaves = random.randint(config.NUM_LEAVES_RANGE[0], config.NUM_LEAVES_RANGE[1])\n",
    "        min_data_in_leaf = random.randint(config.MIN_DATA_IN_LEAF_RANGE[0], config.MIN_DATA_IN_LEAF_RANGE[1])\n",
    "        feature_fraction = random.uniform(config.FEATURE_FRACTION_RANGE[0], config.FEATURE_FRACTION_RANGE[1])\n",
    "\n",
    "        # Define the pipeline with LGBMClassifier\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('scale', StandardScaler(), columns_to_scale),\n",
    "                    ('passthrough', 'passthrough', columns_to_passthrough)\n",
    "                ]\n",
    "            )),\n",
    "            ('lgbm', LGBMClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                num_leaves=num_leaves,\n",
    "                min_data_in_leaf=min_data_in_leaf,\n",
    "                feature_fraction=feature_fraction,\n",
    "                class_weight=class_weights,\n",
    "                random_state=config.RANDOM_STATE,\n",
    "                verbose=-1\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        # Train the model\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to train model on trial #{trial}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate on training data\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_pred_proba = model.predict_proba(X_train)\n",
    "        train_auc = roc_auc_score(y_train, y_train_pred_proba, multi_class='ovo')\n",
    "        train_recall = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        # Evaluate on test data\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred_proba = model.predict_proba(X_test)\n",
    "        test_auc = roc_auc_score(y_test, y_test_pred_proba, multi_class='ovo')\n",
    "        test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        # Store metrics\n",
    "        train_aucs.append(train_auc)\n",
    "        test_aucs.append(test_auc)\n",
    "        train_recalls.append(train_recall)\n",
    "        test_recalls.append(test_recall)\n",
    "\n",
    "        # Clear previous output and display current trial information\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Trial #{trial}\")\n",
    "        if trial > 1:\n",
    "            print(f\"Train ROC AUC Range (Trials 1-{trial-1}): \"\n",
    "                  f\"[{min(train_aucs):.4f}, {max(train_aucs):.4f}]\")\n",
    "            print(f\"Test ROC AUC Range (Trials 1-{trial-1}): \"\n",
    "                  f\"[{min(test_aucs):.4f}, {max(test_aucs):.4f}]\")\n",
    "            print(f\"Train Minority Recall Range (Trials 1-{trial-1}): \"\n",
    "                  f\"[{min(train_recalls):.4f}, {max(train_recalls):.4f}]\")\n",
    "            print(f\"Test Minority Recall Range (Trials 1-{trial-1}): \"\n",
    "                  f\"[{min(test_recalls):.4f}, {max(test_recalls):.4f}]\")\n",
    "\n",
    "        # Update best model if current test AUC is better\n",
    "        if test_auc > best_auc:\n",
    "            best_auc = test_auc\n",
    "            best_model = model\n",
    "            best_params = {\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                'num_leaves': num_leaves,\n",
    "                'min_data_in_leaf': min_data_in_leaf,\n",
    "                'feature_fraction': feature_fraction,\n",
    "                'train_auc': train_auc,\n",
    "                'test_auc': test_auc,\n",
    "                'train_recall': train_recall,\n",
    "                'test_recall': test_recall,\n",
    "                'criteria_met': False\n",
    "            }\n",
    "\n",
    "        # Check if criteria are met\n",
    "        if (train_auc >= config.ACCEPTABLE_TRAIN_AUC_RANGE[0] and \n",
    "            test_auc >= config.ACCEPTABLE_TEST_AUC_RANGE[0] and \n",
    "            test_recall >= config.MIN_RECALL_TEST):\n",
    "            criteria_met = True\n",
    "            best_params['criteria_met'] = True\n",
    "            print(f\"[✓] Criteria met — Train ROC AUC: {train_auc:.4f}, \"\n",
    "                  f\"Test ROC AUC: {test_auc:.4f}, \"\n",
    "                  f\"Train Minority Recall: {train_recall:.4f}, \"\n",
    "                  f\"Test Minority Recall: {test_recall:.4f}\")\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Trial': trial,\n",
    "            'n_estimators': n_estimators,\n",
    "            'learning_rate': learning_rate,\n",
    "            'max_depth': max_depth,\n",
    "            'num_leaves': num_leaves,\n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'feature_fraction': feature_fraction,\n",
    "            'Train ROC AUC': train_auc,\n",
    "            'Test ROC AUC': test_auc,\n",
    "            'Train Recall': train_recall,\n",
    "            'Test Recall': test_recall\n",
    "        })\n",
    "\n",
    "        if trial % 100 == 0:\n",
    "            logger.info(f\"Trial {trial}: Highest Test ROC AUC so far = {best_auc:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    logger.info(\"Random hyperparameter search completed.\")\n",
    "    if criteria_met:\n",
    "        print(f\"[✓] Model meeting criteria found (Test ROC AUC = {best_auc:.4f}).\")\n",
    "    else:\n",
    "        print(f\"[✗] No model met the specified criteria, but the best model (Test ROC AUC = {best_auc:.4f}) was selected.\")\n",
    "    return best_model, best_params, results_df\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Model Evaluation & Feature Importance\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def evaluate_model(best_model, X_train_eval, y_train_eval, X_test, y_test, label_encoder_group, config):\n",
    "    \"\"\"Evaluate the best LightGBM model and return evaluation metrics.\"\"\"\n",
    "    logger.info(\"Evaluating the best LightGBM model\")\n",
    "\n",
    "    # Predictions on training data\n",
    "    y_train_pred = best_model.predict(X_train_eval)\n",
    "    y_train_pred_proba = best_model.predict_proba(X_train_eval)\n",
    "\n",
    "    # Predictions on test data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "    # Evaluation Metrics for Training Data\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train_eval, y_train_pred),\n",
    "        'precision': precision_score(y_train_eval, y_train_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_train_eval, y_train_pred, average='weighted', zero_division=0),\n",
    "        'f1_score': f1_score(y_train_eval, y_train_pred, average='weighted', zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_train_eval, y_train_pred_proba, multi_class='ovo'),\n",
    "        'classification_report': classification_report(y_train_eval, y_train_pred, output_dict=True, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_train_eval, y_train_pred)\n",
    "    }\n",
    "\n",
    "    # Evaluation Metrics for Test Data\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_test, y_test_pred, average='weighted', zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_test_pred, average='weighted', zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_test_pred_proba, multi_class='ovo'),\n",
    "        'classification_report': classification_report(y_test, y_test_pred, output_dict=True, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
    "    }\n",
    "\n",
    "    # Calculate Precision-Recall AUC\n",
    "    train_pr_auc = {}\n",
    "    test_pr_auc = {}\n",
    "    for i in np.unique(y_train_eval):\n",
    "        # Training PR AUC for class i\n",
    "        train_precision_i, train_recall_i, _ = precision_recall_curve(y_train_eval == i, y_train_pred_proba[:, i])\n",
    "        train_pr_auc[i] = auc(train_recall_i, train_precision_i)\n",
    "        # Test PR AUC for class i\n",
    "        test_precision_i, test_recall_i, _ = precision_recall_curve(y_test == i, y_test_pred_proba[:, i])\n",
    "        test_pr_auc[i] = auc(test_recall_i, test_precision_i)\n",
    "\n",
    "    # Average PR AUC\n",
    "    train_metrics['pr_auc'] = np.mean(list(train_pr_auc.values()))\n",
    "    test_metrics['pr_auc'] = np.mean(list(test_pr_auc.values()))\n",
    "\n",
    "    # Log evaluation metrics\n",
    "    logger.info(f\"Training Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "    logger.info(f\"Testing Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    logger.info(\"Train Classification Report:\")\n",
    "    logger.info(train_metrics['classification_report'])\n",
    "    logger.info(\"Test Classification Report:\")\n",
    "    logger.info(test_metrics['classification_report'])\n",
    "    logger.info(f\"Train ROC AUC: {train_metrics['roc_auc']:.4f}\")\n",
    "    logger.info(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    logger.info(f\"Train PR AUC: {train_metrics['pr_auc']:.4f}\")\n",
    "    logger.info(f\"Test PR AUC: {test_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "    # Feature Importance Calculation using LightGBM's built-in feature importance\n",
    "    logger.info(\"Calculating Feature Importance for LightGBM model\")\n",
    "    lgbm_model = best_model.named_steps['lgbm']\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Importance': lgbm_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Normalize importance scores\n",
    "    feature_importances['Importance'] = feature_importances['Importance'] / feature_importances['Importance'].sum()\n",
    "\n",
    "    # Save Feature Importances to CSV\n",
    "    try:\n",
    "        feature_importances.to_csv(config.FEATURE_IMPORTANCES_PATH, index=False)\n",
    "        logger.info(f\"Feature importances saved to {config.FEATURE_IMPORTANCES_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save feature importances: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Plot and save feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))  # Top 20 features\n",
    "    plt.title('Top 20 Feature Importances (LightGBM)')\n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        plt.savefig(config.FEATURE_IMPORTANCES_PLOT_PATH)\n",
    "        plt.close()\n",
    "        logger.info(f\"Feature importances plot saved to {config.FEATURE_IMPORTANCES_PLOT_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save feature importances plot: {e}\")\n",
    "        raise\n",
    "\n",
    "    return train_metrics, test_metrics, y_train_pred, y_train_pred_proba, y_test_pred, y_test_pred_proba\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Cross-Validation for Robust Evaluation\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def cross_validate_model(best_model, X, y, groups, label_encoder_group, config, n_splits=5):\n",
    "    \"\"\"Perform group-aware cross-validation on the best model.\"\"\"\n",
    "    logger.info(\"Performing group-aware cross-validation with Pipeline\")\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    cv_scores = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'roc_auc': [],\n",
    "        'pr_auc': []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Apply SMOTE with Tomek Links\n",
    "        X_train_resampled, y_train_resampled = apply_smote_tomek(X_train_fold, y_train_fold, config.RANDOM_STATE)\n",
    "\n",
    "        # Update class weights for the LGBMClassifier in the pipeline\n",
    "        best_model.named_steps['lgbm'].class_weight = compute_class_weights(y_train_resampled)\n",
    "\n",
    "        # Fit the model\n",
    "        try:\n",
    "            best_model.fit(X_train_resampled, y_train_resampled)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to train model in fold {fold}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate on validation fold\n",
    "        y_val_pred = best_model.predict(X_val_fold)\n",
    "        y_val_pred_proba = best_model.predict_proba(X_val_fold)\n",
    "\n",
    "        # Compute metrics\n",
    "        cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_val_pred))\n",
    "        cv_scores['precision'].append(precision_score(y_val_fold, y_val_pred, average='weighted', zero_division=0))\n",
    "        cv_scores['recall'].append(recall_score(y_val_fold, y_val_pred, average='weighted', zero_division=0))\n",
    "        cv_scores['f1_score'].append(f1_score(y_val_fold, y_val_pred, average='weighted', zero_division=0))\n",
    "\n",
    "        # Compute ROC AUC, handling missing classes\n",
    "        unique_classes = np.unique(y_val_fold)\n",
    "        if len(unique_classes) < 2:\n",
    "            logger.warning(f\"Fold {fold} has only {len(unique_classes)} class(es). Skipping ROC AUC calculation.\")\n",
    "            cv_scores['roc_auc'].append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                class_indices = [np.where(label_encoder_group.classes_ == cls)[0][0] for cls in unique_classes]\n",
    "                y_val_pred_proba_subset = y_val_pred_proba[:, class_indices]\n",
    "                cv_scores['roc_auc'].append(roc_auc_score(y_val_fold, y_val_pred_proba_subset, multi_class='ovo'))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to compute ROC AUC for fold {fold}: {e}\")\n",
    "                cv_scores['roc_auc'].append(np.nan)\n",
    "\n",
    "        # PR AUC for each class present in the fold\n",
    "        pr_auc_fold = []\n",
    "        for i in unique_classes:\n",
    "            precision_i, recall_i, _ = precision_recall_curve(y_val_fold == i, y_val_pred_proba[:, np.where(label_encoder_group.classes_ == i)[0][0]])\n",
    "            pr_auc_fold.append(auc(recall_i, precision_i))\n",
    "        cv_scores['pr_auc'].append(np.mean(pr_auc_fold))\n",
    "\n",
    "        logger.info(f\"Fold {fold} - ROC AUC: {cv_scores['roc_auc'][-1]:.4f}, Recall: {cv_scores['recall'][-1]:.4f}\")\n",
    "\n",
    "    # Compute average metrics, ignoring NaN values\n",
    "    avg_metrics = {key: np.nanmean(values) for key, values in cv_scores.items()}\n",
    "    logger.info(\"Cross-validation results:\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        logger.info(f\"Average {metric}: {value:.4f}\")\n",
    "\n",
    "    # Save CV results\n",
    "    cv_results_df = pd.DataFrame(cv_scores)\n",
    "    cv_results_df['Fold'] = range(1, len(cv_scores['accuracy']) + 1)\n",
    "    try:\n",
    "        cv_results_df.to_csv(config.CV_RESULTS_PATH, index=False)\n",
    "        logger.info(f\"Cross-validation results saved to {config.CV_RESULTS_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save cross-validation results: {e}\")\n",
    "        raise\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Save Model & Results\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def save_model(model, file_path):\n",
    "    \"\"\"Save the trained model to a file.\"\"\"\n",
    "    logger.info(f\"Saving model to {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        logger.info(\"Model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save model: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_results(results_df, file_path):\n",
    "    \"\"\"Save the hyperparameter search results to a CSV file.\"\"\"\n",
    "    logger.info(f\"Saving hyperparameter search results to {file_path}\")\n",
    "    try:\n",
    "        results_df.to_csv(file_path, index=False)\n",
    "        logger.info(\"Hyperparameter search results saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "        raise\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Save Predictions\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def save_predictions(X_train_eval, y_train_eval, y_train_pred, y_train_pred_proba, groups_train, train_idx,\n",
    "                     X_test, y_test, y_test_pred, y_test_pred_proba, groups_test, test_idx,\n",
    "                     df_processed, label_encoder_group, config):\n",
    "    \"\"\"Save training and test data with predictions.\"\"\"\n",
    "    logger.info(\"Saving predictions to CSV files\")\n",
    "\n",
    "    # Training data\n",
    "    train_data = X_train_eval.copy()\n",
    "    train_data['Subject ID'] = groups_train.values\n",
    "    train_data['Visit'] = df_processed['Visit'].iloc[train_idx].values\n",
    "    train_data['Target'] = y_train_eval\n",
    "    train_data['Predicted'] = y_train_pred\n",
    "\n",
    "    # Predicted probabilities\n",
    "    for idx, class_label in enumerate(label_encoder_group.classes_):\n",
    "        train_data[f'Predicted_Prob_{class_label}'] = y_train_pred_proba[:, idx]\n",
    "\n",
    "    # Test data\n",
    "    test_data = X_test.copy()\n",
    "    test_data['Subject ID'] = groups_test.values\n",
    "    test_data['Visit'] = df_processed['Visit'].iloc[test_idx].values\n",
    "    test_data['Target'] = y_test\n",
    "    test_data['Predicted'] = y_test_pred\n",
    "\n",
    "    # Predicted probabilities\n",
    "    for idx, class_label in enumerate(label_encoder_group.classes_):\n",
    "        test_data[f'Predicted_Prob_{class_label}'] = y_test_pred_proba[:, idx]\n",
    "\n",
    "    # Save to CSV\n",
    "    try:\n",
    "        train_data.to_csv(config.TRAIN_PREDICTIONS_PATH, index=False)\n",
    "        logger.info(f\"Training predictions saved to {config.TRAIN_PREDICTIONS_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save training predictions: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        test_data.to_csv(config.TEST_PREDICTIONS_PATH, index=False)\n",
    "        logger.info(f\"Test predictions saved to {config.TEST_PREDICTIONS_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save test predictions: {e}\")\n",
    "        raise\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Save Label Encoder and Feature Names\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def save_label_encoder(label_encoder, file_path):\n",
    "    \"\"\"Save the LabelEncoder object to a file.\"\"\"\n",
    "    logger.info(f\"Saving LabelEncoder to {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "        logger.info(\"LabelEncoder saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save LabelEncoder: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_feature_names(feature_names, file_path):\n",
    "    \"\"\"Save the feature names to a CSV file.\"\"\"\n",
    "    logger.info(f\"Saving feature names to {file_path}\")\n",
    "    try:\n",
    "        pd.DataFrame({'Feature': feature_names}).to_csv(file_path, index=False)\n",
    "        logger.info(\"Feature names saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save feature names: {e}\")\n",
    "        raise\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Save Evaluation Metrics\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def save_evaluation_metrics(train_metrics, test_metrics, cv_metrics, config):\n",
    "    \"\"\"Save evaluation metrics, including cross-validation metrics, to a CSV file.\"\"\"\n",
    "    logger.info(\"Saving evaluation metrics\")\n",
    "    metrics_data = {\n",
    "        'Dataset': ['Training', 'Test', 'Cross-Validation (Avg)'],\n",
    "        'Accuracy': [\n",
    "            train_metrics['accuracy'],\n",
    "            test_metrics['accuracy'],\n",
    "            cv_metrics['accuracy']\n",
    "        ],\n",
    "        'Precision': [\n",
    "            train_metrics['precision'],\n",
    "            test_metrics['precision'],\n",
    "            cv_metrics['precision']\n",
    "        ],\n",
    "        'Recall': [\n",
    "            train_metrics['recall'],\n",
    "            test_metrics['recall'],\n",
    "            cv_metrics['recall']\n",
    "        ],\n",
    "        'F1 Score': [\n",
    "            train_metrics['f1_score'],\n",
    "            test_metrics['f1_score'],\n",
    "            cv_metrics['f1_score']\n",
    "        ],\n",
    "        'ROC AUC': [\n",
    "            train_metrics['roc_auc'],\n",
    "            test_metrics['roc_auc'],\n",
    "            cv_metrics['roc_auc']\n",
    "        ],\n",
    "        'PR AUC': [\n",
    "            train_metrics['pr_auc'],\n",
    "            test_metrics['pr_auc'],\n",
    "            cv_metrics['pr_auc']\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    try:\n",
    "        metrics_df.to_csv(config.METRICS_SAVE_PATH, index=False)\n",
    "        logger.info(f\"Evaluation metrics saved to {config.METRICS_SAVE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save evaluation metrics: {e}\")\n",
    "        raise\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Main\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "def main():\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "\n",
    "    # Load data\n",
    "    df_processed = load_data(config.PROCESSED_DATA_PATH)\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y, groups = preprocess_data(df_processed)\n",
    "\n",
    "    # Encode labels for the full dataset\n",
    "    label_encoder_group = LabelEncoder()\n",
    "    y_encoded = label_encoder_group.fit_transform(y)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, groups_train, groups_test, train_idx, test_idx = split_data(\n",
    "        X, y, groups, config.TEST_SIZE, config.RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Encode training and test labels\n",
    "    y_train_encoded = label_encoder_group.transform(y_train)\n",
    "    y_test_encoded = label_encoder_group.transform(y_test)\n",
    "\n",
    "    # Apply SMOTE with Tomek Links to the training data\n",
    "    X_train_resampled, y_train_resampled = apply_smote_tomek(X_train, y_train_encoded, config.RANDOM_STATE)\n",
    "\n",
    "    # Compute class weights after resampling\n",
    "    class_weights = compute_class_weights(y_train_resampled)\n",
    "\n",
    "    # Run hyperparameter search\n",
    "    best_model, best_params, search_results = random_hyperparameter_search(\n",
    "        X_train_resampled, y_train_resampled, X_test, y_test_encoded, class_weights, config\n",
    "    )\n",
    "\n",
    "    if best_model is not None:\n",
    "        logger.info(f\"Best Model Parameters: {best_params}\")\n",
    "\n",
    "        # Save the best model\n",
    "        save_model(best_model, config.MODEL_SAVE_PATH)\n",
    "\n",
    "        # Save hyperparameter search results\n",
    "        save_results(search_results, config.SEARCH_RESULTS_PATH)\n",
    "\n",
    "        # Evaluate the best model\n",
    "        train_metrics, test_metrics, y_train_pred, y_train_pred_proba, y_test_pred, y_test_pred_proba = evaluate_model(\n",
    "            best_model, X_train, y_train_encoded, X_test, y_test_encoded, label_encoder_group, config\n",
    "        )\n",
    "\n",
    "        # Perform cross-validation\n",
    "        cv_metrics = cross_validate_model(\n",
    "            best_model, X, y_encoded, groups, label_encoder_group, config, n_splits=config.CV_FOLDS\n",
    "        )\n",
    "\n",
    "        # Save predictions\n",
    "        save_predictions(\n",
    "            X_train, y_train_encoded, y_train_pred, y_train_pred_proba, groups_train, train_idx,\n",
    "            X_test, y_test_encoded, y_test_pred, y_test_pred_proba, groups_test, test_idx,\n",
    "            df_processed, label_encoder_group, config\n",
    "        )\n",
    "\n",
    "        # Save LabelEncoder\n",
    "        save_label_encoder(label_encoder_group, config.LABEL_ENCODER_PATH)\n",
    "\n",
    "        # Save feature names\n",
    "        save_feature_names(X_train.columns, config.FEATURE_NAMES_PATH)\n",
    "\n",
    "        # Save evaluation metrics\n",
    "        save_evaluation_metrics(train_metrics, test_metrics, cv_metrics, config)\n",
    "\n",
    "        # Print final metrics\n",
    "        print(f\"Best Model Metrics (Single Split):\")\n",
    "        print(f\"Train ROC AUC: {train_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"Train Recall: {train_metrics['recall']:.4f}\")\n",
    "        print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"\\nCross-Validation Metrics (Average over {config.CV_FOLDS} folds):\")\n",
    "        for metric, value in cv_metrics.items():\n",
    "            print(f\"Average {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        logger.warning(\"No valid models were trained during hyperparameter search.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
